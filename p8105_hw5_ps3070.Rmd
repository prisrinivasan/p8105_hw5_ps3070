---
title: "p8105_hw5_ps3070"
output: github_document
editor_options: 
  chunk_output_type: inline
---

Loading the tidyverse, knitr, and patchwork packages:

```{r setup, message=FALSE, warning=FALSE}
library(tidyverse)
library(knitr)
library(patchwork)

options(ggplot2.continuous.colour = "viridis",
        ggplot2.continuous.fill = "viridis")

scale_colour_discrete = scale_colour_viridis_d()
scale_fill_discrete = scale_fill_viridis_d()

theme_set(theme_light() + theme(legend.position = "bottom"))
```


## Problem 1

Loading the iris dataset using code from instructions, and confirming that it is a list:

```{r iris data, message=FALSE, warning=FALSE}
set.seed(10)

iris_with_missing = iris %>% 
  map_df(~replace(.x, sample(1:150, 20), NA)) %>%
  mutate(Species = as.character(Species))

is.list(iris_with_missing) #confirmed that it is a list
```

Below, I wrote a function that does the following:

  * For numeric variables, I filled in missing values with the mean of non-missing values
  * For character variables, I filled in missing values with "virginica"

I also applied this function to the columns of `iris_with_missing` using a map statement. The table printed underneath the code chunk shows the first few results after the function is applied.

```{r iris missing, message=FALSE, warning=FALSE}
replace_iris_missing = function(a) {
  if (is.numeric(a)) {
    b = mean(a, na.rm = TRUE) #creating new variable for means
    a = replace(a, is.na(a), b) #replacing na values in a with means from b
  } else if (is.character(a)) {
    a = replace(a, is.na(a), "virginica") #replacing character na values
  }
}

output = map_df(iris_with_missing, replace_iris_missing) #maps function to each vector 

head(output) %>% 
  kable(format = "html", caption = "Iris Dataset without Missing")
```

## Problem 2

This problem uses data from a longitudinal study with control and experimental arms. I loaded the data from each separate file, consolidated into one data frame, and tidied the data frame to include all observations, subject ID, arm, and a time variable (week). 

```{r make tibble of file names, message=FALSE, warning=FALSE}
names = tibble(
  file_names = list.files(path = "./data") #creating vector file_names
  )

names #created a df with file names called names
```

There are `r names %>% filter(grepl("con", pull(names, file_names))) %>% nrow()` patients in the control arm and 
`r names %>% filter(grepl("exp", pull(names, file_names))) %>% nrow()` patients in the experimental arm. These file names are now stored in one column in the `names` data frame.

```{r function to get data from "data" directory, message=FALSE, warning=FALSE}
get_data = function(x) {
  
  {directory = "./data/"
  data = read.csv(paste(directory, x, sep = ""))}
  
  return(data)
}
```

The code chunk below is where I iterated over file names to read in data for each subject using `map` and tidied the dataset. The first few results are shown in the table below.

```{r}
tidy_df = names %>% 
  mutate(obs = map(file_names, get_data)) %>% 
  unnest(obs) %>%
  janitor::clean_names() %>% 
  mutate(subject = str_replace(file_names, ".csv", ""),
         arm = str_replace(file_names, "_.*", ""),
         arm = recode(arm, "con" = "control", "exp" = "experimental")) %>% 
  gather(key = week, value = measurement, week_1:week_8) %>%
  separate(week, into = c("extra", "week"), sep = "_") %>% 
  mutate(week = as.numeric(week)) %>% 
  select(-extra, -file_names)

head(tidy_df) %>%
  kable(format = "html", caption = "Study Measurements") #tibble sorted by week
```


Using the tidied dataset, I created a spaghetti plot showing observations on each subject over time, with the control group shown in red and the experimental group shown in blue. Here we can see that the observations/measurements are overall higher for the experimental group, and it seems like they trend upwards (increase over time) in the experimental group. The control group generally has lower measurements and based on this plot, it is unclear whether it trends up or downwards. Overall, there also seems to be a lot of fluctuation between weeks.

```{r spaghetti plot}
files_plot = tidy_df %>% 
  ggplot(aes(x = week, y = measurement, color = arm, group = subject)) +
  geom_line() +
  labs(x = "Week",
       y = "Measurement",
       title = "Observations by Week based on Study Arm") +
  scale_x_continuous(breaks = c(1:8))

files_plot
```

## Problem 3

First, I set the seed for reproducibility and set the following design elements:

```{r set design elements, message=FALSE, warning=FALSE}
set.seed(1)

n = 30
x_i1 = rnorm(n, 0, 1) #x draws from standard normal dist so mean is 0 and sd is 1
beta0 = 2
sd = 50 

beta1 = 0
```

Then, I am generating 10,000 datasets from the following model:

$$ y_i = \beta_0 + \beta_1 x_{i1} + \epsilon_{i} $$

with $\epsilon_{i} âˆ¼ N[0,\sigma^2]$.

Below, I created a simulation function that can do this for me.

```{r creating function, message=FALSE, warning=FALSE}
sim = function(n = 30, beta0 = 2, beta1 = 0) {
  
  sim_data = tibble(
    x_i1 = rnorm(n, mean = 0, sd = 1),
    y = beta0 + beta1 * x_i1 + rnorm(n, 0, 50)
  )
  
  ls_fit = lm(y ~ x_i1, data = sim_data) %>% 
    broom::tidy() %>% 
    filter(term == "x_i1") %>%
    select("estimate", "p.value")
}

```

Below I ran a simulation to generate 10,000 Beta 1 hat values and p-values using an alpha of 0.05 (5% significance level) and based on a test of
$$ H : \beta_1 = 0 $$

```{r simulation, message=FALSE, warning=FALSE}
simulation =
  rerun(10000, sim(n, beta0, beta1)) %>% 
  bind_rows()
```


Next, I repeated this simulation for $\beta_1 = \{1, 2, 3, 4, 5, 6\}$. The table generated in the code chunk below (for the first few results)  returns estimates and p-values, simulated 10,000 times for each beta.


```{r simulate all beta1s, message=FALSE, warning=FALSE}
beta1_sim = 
  tibble(betas = c(0:6)) %>% #created list adding betas from 1-6 to the previous 
  mutate(
    sim_output = map(.x = betas, ~ rerun(10000, sim(n, beta0, beta1 = .x))), #used .x for betas list and mapped the simulation in prev code chunk
    estimates = map(sim_output, bind_rows)) %>%
  select(-sim_output) %>% 
  unnest(estimates)

head(beta1_sim) %>% 
  kable(format = "html", caption = "Beta and P-value Simulation")
```

Below is a plot showing the proportion of times the null was rejected (power) on the y axis and the true value of $\beta_1$  on the x axis. As seen in this plot, as effect size increases (i.e. when $\beta_1$ increases), power also increases. The association is not linear, but it could be exponential or logarithmic.

```{r power plot, message=FALSE, warning=FALSE}
power_plot = beta1_sim %>% 
  group_by(betas) %>% 
  summarise(n = n(), #total count
            p = sum(p.value < 0.05), #<0.05 rejects null
            power = p/n) %>% #to show proportion of times null was rejected
  ggplot(aes(x = betas, y = power, color = power)) +
  geom_point(show.legend = FALSE) +
  geom_line(show.legend = FALSE) +
  labs(title = "Effect Size vs Power", 
       x = "Effect Size", 
       y = "Power") +
  scale_x_continuous(breaks = c(1:6))

power_plot
```

Below is are two plots:

  * Violet: average estimate of Beta 1 Hat on y axis and the true value of $\beta_1$ on the x-axis
  * Blue: average estimate of Beta 1 Hat *only where null was rejected* (p < 0.05) on the y-axis and the true value of $\beta_1$ on the x-axis

```{r overlay plots, message=FALSE, warning=FALSE}
overall_plot = beta1_sim %>% 
  group_by(betas) %>% 
  summarise(mean_beta1 = mean(estimate))

rej_plot = beta1_sim %>% 
  filter(p.value < 0.05) %>% #filter where null is rejected
  group_by(betas) %>% 
  summarise(mean_rej_beta1 = mean(estimate))

overlay =
  ggplot() +
  geom_point(data = overall_plot, aes(x = betas, y = mean_beta1), color = "blue") +
  geom_line(data = overall_plot, aes(x = betas, y = mean_beta1), color = "blue") +
  geom_point(data = rej_plot, aes(x = betas, y = mean_rej_beta1), color = "violet") +
  geom_line(data = rej_plot, aes(x = betas, y = mean_rej_beta1), color = "violet") +
  labs(x = "True Beta 1 Values",
       y = "Average Estimates of Beta 1 Hat",
       title = "Average Beta 1 Hat") +
  scale_x_continuous(breaks = c(0:6))

overlay
```

Both plots above show a positive correlation between true $\beta_1$ values and Beta 1 Hat estimates. The sample average of Beta 1 Hat across tests for which the null is rejected is not approximately equal to the true value of $\beta_1$. This could be because rejected p-values (higher p-values) are more likely to have a larger effect size, so estimates of Beta 1 Hat will be larger compared to the true $\beta_1$ values. 
